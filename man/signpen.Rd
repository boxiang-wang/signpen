% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/signpen.R
\name{signpen}
\alias{signpen}
\title{perceptron penalty for sign consistency}
\usage{
signpen(x, y, method = c("ls", "logit"), nlambda = 100,
  lambda.factor = ifelse(nobs < nvars, 0.01, 1e-04), lambda = NULL,
  lambda2 = 0, lambda3 = 1, gam = rep(1, nvars), pf = rep(1,
  nvars), pf2 = rep(1, nvars), exclude, dfmax = nvars + 1,
  pmax = min(dfmax * 1.2, nvars), standardize = TRUE, eps = 1e-08,
  maxit = 1e+06)
}
\arguments{
\item{x}{A matrix with \eqn{N} rows and \eqn{p} columns for predictors.}

\item{y}{A vector of length \eqn{p} for responses.}

\item{method}{A character string specifying the loss function to use. 
Options are 
  "ls": least squared regression;
  "logit": logistic regression.
Default is "ls".}

\item{nlambda}{The number of \code{lambda} values, 
i.e., length of the \code{lambda} sequence. Default is 100.}

\item{lambda.factor}{Takes the value of 0.0001 if \eqn{N >= p} or 0.01 if \eqn{N < p}.
Takes no effect when user specifies a \code{lambda} sequence.}

\item{lambda}{An optional user-supplied \code{lambda} sequence. 
If \code{lambda = NULL} (default), the program computes 
its own \code{lambda} sequence based on \code{nlambda} and \code{lambda.factor}; 
otherwise, the program uses the user-specified one. 
Since the program will automatically sort user-defined \code{lambda} sequence in decreasing order, 
it is better to supply a decreasing sequence.}

\item{lambda2}{The L2 tuning parameter \eqn{\lambda_2}{lambda2}.}

\item{lambda3}{The L3 tuning parameter \eqn{\lambda_3}{lambda3}.}

\item{gam}{The gamma vector of length \eqn{p} for prior information on the signs of \code{beta}.}

\item{pf}{A vector of length \eqn{p}{p} representing the L1 penalty weights 
\code{pf} can be 0 for some predictor(s), leading to including the predictor(s) all the time. 
 One suggested choice of \code{pf} is \eqn{{(\beta + 1/n)}^{-1}}{1/(beta+1/n)}, 
 where \eqn{n} is the sample size and \eqn{\beta}{beta} is the coefficents obtained. 
 Default is 1 for all predictors (and infinity if some predictors are listed in \code{exclude}).}

\item{pf2}{A vector of length \eqn{p}{p} for L2 penalty factor for adaptive L1 or adaptive elastic net. 
To allow different L2 shrinkage, user can set \code{pf2} to be different L2 penalty weights 
for each coefficient of \eqn{\beta}{beta}. \code{pf2} can be 0 for some variables, 
indicating no L2 shrinkage. Default is 1 for all predictors.}

\item{exclude}{Whether to exclude some predictors from the model. 
This is equivalent to adopting an infinite penalty factor when excluding some predictor. 
Default is none.}

\item{dfmax}{Restricts at most how many predictors can be incorporated in the model. Default is \eqn{p+1}. 
This restriction is helpful when \eqn{p} is large, provided that a partial path is acceptable.}

\item{pmax}{Restricts the maximum number of variables ever to be nonzero; 
e.g, once some \eqn{\beta} enters the model, it counts once. 
The count will not change when the \eqn{\beta} exits or re-enters the model. 
Default is \code{min(dfmax*1.2,p)}.}

\item{standardize}{Whether to standardize the data. If \code{TRUE}, 
\code{\link{signpen}} normalizes the predictors such that each column has 
sum squares\eqn{\sum^N_{i=1}x_{ij}^2/N=1}{<Xj,Xj>/N} of one. 
Note that x is always centered (i.e. \eqn{\sum^N_{i=1}x_{ij}=0}{sum(Xj)=0}) 
no matter \code{standardize} is \code{TRUE} or \code{FALSE}. 
\code{\link{signpen}} always returns coefficient \code{beta} on the original scale.  
Default value is \code{TRUE}.}

\item{eps}{The algorithm stops when 
(i.e. \eqn{\max_j(\beta_j^{new}-\beta_j^{old})^2}{max(j)(beta_new[j]-beta_old[j])^2} 
is less than \code{eps}, where \eqn{j=0,\ldots, p}. Defaults value is \code{1e-8}.}

\item{maxit}{Restricts how many outer-loop iterations are allowed. 
Default is 1e6. Consider increasing \code{maxit} when the algorithm does not converge.}
}
\value{
An object with S3 class \code{\link{signpen}}.
 \item{b0}{A vector of length \code{length(lambda)} representing the intercept at each \code{lambda} value.}
 \item{beta}{A matrix of dimension \code{p*length(lambda)} representing the coefficients at each \code{lambda} value. 
  The matrix is stored as a sparse matrix  (\code{Matrix} package). 
    To convert it into normal type matrix use \code{as.matrix()}.}
 \item{df}{The number of nonzero coefficients at each \code{lambda}.}
 \item{dim}{The dimension of coefficient matrix, i.e., \code{p*length(lambda)}.}
 \item{lambda}{The \code{lambda} sequence that was actually used.}
 \item{npasses}{Total number of iterations for all lambda values. }
 \item{jerr}{Warnings and errors; 0 if no error.}
 \item{call}{The call that produced this object.}
}
\description{
perceptron penalty for sign consistency
}
\details{
The \code{\link{signpen}} minimizes the sparse penalized loss function with the perceptron penalty, 
  \deqn{L(y, X, \beta)/N + \lambda_1||\beta||_1 + 0.5\lambda_2||\beta||_2^2 + \lambda_3 (-\beta_j \gamma_j)_+. }
  {L(y, X, beta))/N + lambda1 * ||beta||_1^1 + 0.5 * lambda2 * ||beta||_2^2 + lambda3 (-beta_j gamma_j)_+. }
  The values of \code{lambda2} \code{lambda3} are user-specified.
  
  When the algorithm do not converge or run slow, consider increasing \code{eps}, decreasing
  \code{nlambda}, or increasing \code{lambda.factor} before increasing
  \code{maxit}.
}
\keyword{consistency}
\keyword{penalty}
\keyword{perceptron}
\keyword{sign}
